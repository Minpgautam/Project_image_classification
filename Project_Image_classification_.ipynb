{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Data\n",
    "Run the following cell to download the CIFAR-10 dataset for python\n",
    "`(https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)`.\n",
    "### Data\n",
    "CIFAR-10 is an established computer-vision dataset used for object recognition. It is a subset of the\n",
    "80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10\n",
    "object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and\n",
    "Geoffrey Hinton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessay library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "class DLProgress(tqdm):\n",
    "    #download progress\n",
    "    last_block = 0\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        \n",
    "        urlretrieve('https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "                    'cifar-10-python.tar.gz',pbar.hook)\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of the labels as mentioned in file and sites.\n",
    "def load_label_names():\n",
    "    # load the names from the files\n",
    "    return ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the batch of the dataset. The dataset is broken into batches to prevent running of memory.\n",
    "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    \"\"\"\n",
    "    Load a batch of the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "              # Here encoding data type is latin1\n",
    "              batch = pickle.load(file, encoding='latin1')\n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0,2,3,1)\n",
    "    labels = batch['labels']\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n",
    "    \n",
    "    \"\"\"\n",
    "     Display Stats of the the dataset\n",
    "    \"\"\"\n",
    "    batch_ids = list(range(1, 6))   # we've batch number from 0 to 5\n",
    "    \n",
    "    if batch_id not in batch_ids:\n",
    "        print('Batch Id out of Range. Possible Batch Ids: {}'.format(batch_ids))\n",
    "        return None\n",
    "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
    "    if not (0 <= sample_id < len(features)):\n",
    "        print('{} samples in batch {}. {} is out of range.'.format(len(features),batch_id,\n",
    "                                                                   sample_id))\n",
    "        return None\n",
    "    print('\\nStats of batch {}:'.format(batch_id))\n",
    "    print('Samples: {}'.format(len(features)))\n",
    "    print('Label Counts: {}'.format(dict(zip(*np.unique(labels, return_counts=True)))))\n",
    "    print('First 20 Labels: {}'.format(labels[:20]))\n",
    "    # to make look look in print let do this\n",
    "    label_names=load_label_names()\n",
    "    label_counts=dict(zip(*np.unique(labels,return_counts=True)))\n",
    "    for key, value in label_counts.items():\n",
    "        print(\"Label Counts: [{}][ Label{}]  := {}\". format(key,label_names[key].upper(),value))\n",
    "    sample_image = features[sample_id]\n",
    "    sample_label = labels[sample_id]\n",
    "    label_names = load_label_names()\n",
    "    print('\\nExample of Image {}:'.format(sample_id))\n",
    "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
    "    print('Image - Shape: {}'.format(sample_image.shape))\n",
    "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll preprocess the images, then train a convolutional neural network on all the samples.\n",
    "The images need to be normalized and the labels need to be one-hot encoded. You'll get to apply\n",
    "what you learned and build a convolutional, max pooling, dropout, and fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the image data\n",
    "def normalize(x):\n",
    "    # x is the input image array [rows,columns,RGB]\n",
    "    min_val=np.min(x)\n",
    "    max_val=np.max(x)\n",
    "    x=(x - min_val)/(max_val - min_val)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels also need to be one hot encoded\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_binarizer=preprocessing.LabelBinarizer()\n",
    "label_binarizer.fit(range(10))\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    # x is the lists of labels which is encoded matrix to (no.of labels,no.of class)\n",
    "    return label_binarizer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
    "    \n",
    "    \"\"\"\n",
    "     Preprocess data and save it to file\n",
    "    \"\"\"\n",
    "    features = normalize(features)\n",
    "    labels = one_hot_encode(labels)\n",
    "    pickle.dump((features, labels), open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
    "    \"\"\"\n",
    "       Preprocess Training and Validation Data\n",
    "    \"\"\"\n",
    "    n_batches = 5\n",
    "    valid_features = []\n",
    "    valid_labels = []\n",
    "    for batch_i in range(1, n_batches + 1):\n",
    "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
    "        validation_count = int(len(features) * 0.1)   # 10% batch of whole dataset\n",
    "# Prprocess and save a batch of training data\n",
    "        _preprocess_and_save(normalize,one_hot_encode,features[:-validation_count],\n",
    "                     labels[:-validation_count],'preprocess_batch_' + str(batch_i) + '.p')\n",
    "         # here normalizing,onehotencoding,saving in new file named \"preprocess_batch\" +\n",
    "                                                                  #    \" batch number\"\n",
    "# Use a portion of training batch for validation\n",
    "        valid_features.extend(features[-validation_count:])\n",
    "        valid_labels.extend(labels[-validation_count:])\n",
    "# Preprocess and Save all validation data\n",
    "    _preprocess_and_save(normalize,\n",
    "                         one_hot_encode,\n",
    "                         np.array(valid_features),\n",
    "                         np.array(valid_labels),\n",
    "                         'preprocess_validation.p')\n",
    "    # loading the test dataset\n",
    "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
    "        \n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "# preprocess\n",
    "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0,2,3,1)\n",
    "    test_labels = batch['labels']\n",
    "# Preprocess and Save all testing data\n",
    "    _preprocess_and_save(normalize,\n",
    "                         one_hot_encode,\n",
    "                         np.array(test_features),\n",
    "                         np.array(test_labels),\n",
    "                         'preprocess_training.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save_data(cifar10_dataset_folder_path,normalize,one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "     Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "     Load the Preprocessed Training data and return them in batches of <batch_size\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = pickle.load(open(filename, mode='rb'))\n",
    "# Return the training data in batches of size <batch_size> or less\n",
    "    return batch_features_labels(features, labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_predictions(features, labels, predictions):\n",
    "    n_classes = 10\n",
    "    label_names = _load_label_names()\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(range(n_classes))\n",
    "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
    "    fig, axies = plt.subplots(nrows=4, ncols=2)\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n",
    "    n_predictions = 3\n",
    "    margin = 0.05\n",
    "    ind = np.arange(n_predictions)\n",
    "    width = (1. - 2. * margin) / n_predictions\n",
    "    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features,\n",
    "                                                                                 label_ids,\n",
    "                                                                                 predictions.indices,\n",
    "                                                                                 predictions.values)):\n",
    "        \n",
    "        pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
    "        correct_name = label_names[label_id]\n",
    "        axies[image_i][0].imshow(feature*255)\n",
    "        axies[image_i][0].set_title(correct_name)\n",
    "        axies[image_i][0].set_axis_off()\n",
    "        axies[image_i][1].barh(ind + margin, pred_values[::-1], width)\n",
    "        axies[image_i][1].set_yticks(ind + margin)\n",
    "        axies[image_i][1].set_yticklabels(pred_names[::-1])\n",
    "        axies[image_i][1].set_xticks([0, 0.5, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 3:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n",
      "First 20 Labels: [8, 5, 0, 6, 9, 2, 8, 3, 6, 2, 7, 4, 6, 9, 0, 0, 7, 3, 7, 2]\n",
      "Label Counts: [0][ LabelAIRPLANE]  := 994\n",
      "Label Counts: [1][ LabelAUTOMOBILE]  := 1042\n",
      "Label Counts: [2][ LabelBIRD]  := 965\n",
      "Label Counts: [3][ LabelCAT]  := 997\n",
      "Label Counts: [4][ LabelDEER]  := 990\n",
      "Label Counts: [5][ LabelDOG]  := 1029\n",
      "Label Counts: [6][ LabelFROG]  := 978\n",
      "Label Counts: [7][ LabelHORSE]  := 1015\n",
      "Label Counts: [8][ LabelSHIP]  := 961\n",
      "Label Counts: [9][ LabelTRUCK]  := 1029\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 9 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 5\n",
    "display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 5:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1014, 1: 1014, 2: 952, 3: 1016, 4: 997, 5: 1025, 6: 980, 7: 977, 8: 1003, 9: 1022}\n",
      "First 20 Labels: [1, 8, 5, 1, 5, 7, 4, 3, 8, 2, 7, 2, 0, 1, 5, 9, 6, 2, 0, 8]\n",
      "Label Counts: [0][ LabelAIRPLANE]  := 1014\n",
      "Label Counts: [1][ LabelAUTOMOBILE]  := 1014\n",
      "Label Counts: [2][ LabelBIRD]  := 952\n",
      "Label Counts: [3][ LabelCAT]  := 1016\n",
      "Label Counts: [4][ LabelDEER]  := 997\n",
      "Label Counts: [5][ LabelDOG]  := 1025\n",
      "Label Counts: [6][ LabelFROG]  := 980\n",
      "Label Counts: [7][ LabelHORSE]  := 977\n",
      "Label Counts: [8][ LabelSHIP]  := 1003\n",
      "Label Counts: [9][ LabelTRUCK]  := 1022\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 1 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 7 Name: horse\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "batch_id = 5\n",
    "sample_id = 5\n",
    "display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'preprocess_validataion.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-566316a42fb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvalid_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_labels\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'preprocess_validataion.p'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'preprocess_validataion.p'"
     ]
    }
   ],
   "source": [
    "valid_features,valid_labels= pickle.load(open('preprocess_validataion.p',mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First reset the graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32,shape= (None, 32,32,3),name='inputx') # 32x32 pixel, 3=RGB\n",
    "y = tf.placeholder(tf.float32,shape=(None,10), name='outputy')\n",
    "keep_prob = tf.placeholder(tf.float32,name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Convolutional Model\n",
    "# Note : the entire model consists of 14 layers in total. The technique to layer to build it as:\n",
    "def conv_net(x, keep_prob):\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    # convolution with 64 different filters in size 3x3\n",
    "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "     # convolution with 128 different filters in size 3x3\n",
    "    conv3_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 128, 256], mean=0, stddev=0.08))\n",
    "     # convolution with 256 different filters in size 3x3\n",
    "    conv4_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 256, 512], mean=0, stddev=0.08))\n",
    "     # convolution with 512 different filters in size 3x3\n",
    "    #Signature: tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32,\n",
    "                                                                   #seed=None, name=None)\n",
    "\n",
    "#Outputs random values from a truncated normal distribution.\n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "     # ReLU activation function\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "     # max Pooling by 2\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "   # batch normalization\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "\n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    # ReLU activation function\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    # max Pooling by 2\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') \n",
    "     \n",
    "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "  \n",
    "    # 5, 6\n",
    "    conv3 = tf.nn.conv2d(conv2_bn, conv3_filter, strides=[1,1,1,1], padding='SAME')\n",
    "     # ReLU activation function\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "      # max Pooling by 2\n",
    "    conv3_pool = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') \n",
    "   \n",
    "    conv3_bn = tf.layers.batch_normalization(conv3_pool)\n",
    "    \n",
    "    # 7, 8\n",
    "    conv4 = tf.nn.conv2d(conv3_bn, conv4_filter, strides=[1,1,1,1], padding='SAME')\n",
    "     # ReLU activation function\n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "     # max Pooling by 2\n",
    "    conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    conv4_bn = tf.layers.batch_normalization(conv4_pool) # batch normalization\n",
    "    \n",
    "    # 9\n",
    "    #Flattening the 3D output of the last convolution operations.\n",
    "    flat = tf.contrib.layers.flatten(conv4_bn)  \n",
    "\n",
    "    # 10\n",
    "    \n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "    # Fully Connected layer with 128 units\n",
    "    full1 = tf.nn.dropout(full1, keep_prob)    # dropout\n",
    "    # Batch Normalization\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "    \n",
    "    # 11\n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "    # Fully Connected layer with 256 units\n",
    "    full2 = tf.nn.dropout(full2, keep_prob)  # dropout\n",
    "    # Batch Normalization\n",
    "    full2 = tf.layers.batch_normalization(full2)\n",
    "    \n",
    "    # 12\n",
    "    full3 = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=512, activation_fn=tf.nn.relu)\n",
    "    # Fully Connected layer with 512 units\n",
    "    full3 = tf.nn.dropout(full3, keep_prob)  # dropout\n",
    "    # Batch Normalization\n",
    "    full3 = tf.layers.batch_normalization(full3)    \n",
    "    \n",
    "    # 13\n",
    "    full4 = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=1024, activation_fn=tf.nn.relu)\n",
    "    # Fully Connected layer with 1024 units\n",
    "    full4 = tf.nn.dropout(full4, keep_prob) # dropout\n",
    "    # Batch Normalization\n",
    "    full4 = tf.layers.batch_normalization(full4)        \n",
    "    \n",
    "    # 14\n",
    "    out = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=10, activation_fn=None)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10   # no. of iteration until the network stops learning or start overfitting\n",
    "batch_size = 128    # highest no. that machine has memory has.\n",
    "keep_probability = 0.7  # probability of keeping a node using dropout\n",
    "learning_rate = 0.001  # it should be very low. Number how fast the model learns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-66-057d8c555267>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The cost function and optimizer:\n",
    "logits = conv_net(x, keep_prob)\n",
    "model = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "# cost=> return reduce the tensor\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "# optimizer=> an operation that applies the specified gradients.\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')#return reducedtensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Optimization:\n",
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer, \n",
    "                feed_dict={\n",
    "                    x: feature_batch,\n",
    "                    y: label_batch,\n",
    "                    keep_prob: keep_probability\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing Stats\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    loss = sess.run(cost, \n",
    "                    feed_dict={\n",
    "                        x: feature_batch,\n",
    "                        y: label_batch,\n",
    "                        keep_prob: 1.\n",
    "                    })\n",
    "    valid_acc = sess.run(accuracy, \n",
    "                         feed_dict={\n",
    "                             x: valid_features,\n",
    "                             y: valid_labels,\n",
    "                             keep_prob: 1.\n",
    "                         })\n",
    "    \n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2250 Validation Accuracy: 0.196600\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     1.8468 Validation Accuracy: 0.250000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.6397 Validation Accuracy: 0.279200\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.6618 Validation Accuracy: 0.340800\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.4772 Validation Accuracy: 0.392200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.5134 Validation Accuracy: 0.456000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.3460 Validation Accuracy: 0.429200\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.1164 Validation Accuracy: 0.507800\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.0843 Validation Accuracy: 0.576400\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     0.7971 Validation Accuracy: 0.606200\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     0.9008 Validation Accuracy: 0.612600\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     0.6410 Validation Accuracy: 0.639800\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     0.4584 Validation Accuracy: 0.637600\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     0.5232 Validation Accuracy: 0.677000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     0.4769 Validation Accuracy: 0.682200\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     0.5490 Validation Accuracy: 0.664400\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     0.3519 Validation Accuracy: 0.677000\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     0.2777 Validation Accuracy: 0.695800\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     0.2550 Validation Accuracy: 0.676800\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     0.2472 Validation Accuracy: 0.702000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     0.2997 Validation Accuracy: 0.672400\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     0.2252 Validation Accuracy: 0.719000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     0.0959 Validation Accuracy: 0.710600\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     0.1278 Validation Accuracy: 0.737800\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     0.1542 Validation Accuracy: 0.676800\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     0.1588 Validation Accuracy: 0.713000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     0.1166 Validation Accuracy: 0.735000\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     0.0663 Validation Accuracy: 0.727800\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     0.0550 Validation Accuracy: 0.735400\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     0.0521 Validation Accuracy: 0.722400\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     0.0519 Validation Accuracy: 0.731400\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     0.0520 Validation Accuracy: 0.743000\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.0311 Validation Accuracy: 0.720600\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     0.0621 Validation Accuracy: 0.728400\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     0.0273 Validation Accuracy: 0.736000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.0265 Validation Accuracy: 0.728600\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     0.0228 Validation Accuracy: 0.731600\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.0161 Validation Accuracy: 0.729200\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.0319 Validation Accuracy: 0.729600\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     0.0242 Validation Accuracy: 0.726800\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.0240 Validation Accuracy: 0.725600\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.0144 Validation Accuracy: 0.734600\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.0388 Validation Accuracy: 0.729400\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.0205 Validation Accuracy: 0.723800\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.0073 Validation Accuracy: 0.738600\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.0076 Validation Accuracy: 0.737200\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     0.0110 Validation Accuracy: 0.737200\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.0121 Validation Accuracy: 0.726200\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.0152 Validation Accuracy: 0.745600\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.0139 Validation Accuracy: 0.731400\n"
     ]
    }
   ],
   "source": [
    "save_model_path = './image_classification'\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "                \n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkPoint: The model has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
